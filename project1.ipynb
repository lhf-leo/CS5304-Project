{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.pipeline import Pipeline\n",
    "import dynet as dy\n",
    "import string\n",
    "import random\n",
    "\n",
    "\n",
    "PAD_TOKEN = '_PAD_'\n",
    "UNK_TOKEN = '_UNK_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_data():\n",
    "    data = []\n",
    "    with open('dataset/review.json') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    dataframe = pd.DataFrame(data)\n",
    "\n",
    "    df = dataframe[['text','stars']]\n",
    "    train, validate, test = np.split(df.sample(frac=.20), [int(.15*len(df)), int(.175*len(df))])\n",
    "\n",
    "    pickle.dump(test, open('data/test.dat', 'wb'))\n",
    "    pickle.dump(validate, open('data/validate.dat', 'wb'))\n",
    "    pickle.dump(train, open('data/train.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pickle.load(open('data/train.dat', 'rb'))\n",
    "validate = pickle.load(open('data/validate.dat', 'rb'))\n",
    "test = pickle.load(open('data/test.dat', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarize(data):\n",
    "    if 'stars' not in data.columns: return\n",
    "    data.loc[:, 'positive'] = data.stars.apply(lambda x: 1 if x >=4 else 0)\n",
    "    del data['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarize(train)\n",
    "binarize(validate)\n",
    "binarize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df2list(data):\n",
    "    examples=[]\n",
    "    for _, example in data.iterrows():\n",
    "        d=dict()\n",
    "        table = str.maketrans({key: None for key in string.punctuation})\n",
    "        d['tokens'] = example['text'].lower().translate(table).split()\n",
    "        d['label'] = example['positive']\n",
    "        examples.append(d)\n",
    "    return examples\n",
    "train = df2list(train)\n",
    "validate = df2list(validate)\n",
    "test = df2list(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 692708\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(datasets):\n",
    "    vocab = dict()\n",
    "    vocab[PAD_TOKEN] = len(vocab)\n",
    "    vocab[UNK_TOKEN] = len(vocab)\n",
    "    for data in datasets:\n",
    "        for example in data:\n",
    "            for word in example['tokens']:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = len(vocab)\n",
    "\n",
    "    print('Vocab size: {}'.format(len(vocab)))\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab([train, validate, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TokenConverter(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.unknown = 0\n",
    "\n",
    "    def convert(self, token):\n",
    "        if token in self.vocab:\n",
    "            id = self.vocab.get(token)\n",
    "        else:\n",
    "            id = self.vocab.get(UNK_TOKEN)\n",
    "            self.unknown += 1\n",
    "        return id\n",
    "\n",
    "\n",
    "def convert2ids(data, vocab):\n",
    "    converter = TokenConverter(vocab)\n",
    "    for example in data:\n",
    "        example['tokens'] = list(map(converter.convert, example['tokens']))\n",
    "    print('Found {} unknown tokens.'.format(converter.unknown))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 unknown tokens.\n",
      "Found 0 unknown tokens.\n",
      "Found 0 unknown tokens.\n"
     ]
    }
   ],
   "source": [
    "train = convert2ids(train, vocab)\n",
    "validate = convert2ids(validate, vocab)\n",
    "test = convert2ids(test, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading embeddings from glove.42B.300d.txt\n",
      "Cached embeddings to glove.42B.300d.txt.cache\n"
     ]
    }
   ],
   "source": [
    "def load_embeddings(path, vocab, cache=False, cache_path=None):\n",
    "    if not path: return None\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    if cache_path is None:\n",
    "        cache_path = path + '.cache'\n",
    "\n",
    "    # Use cache file if it exists.\n",
    "    if os.path.exists(cache_path):\n",
    "        path = cache_path\n",
    "\n",
    "    print(\"Reading embeddings from {}\".format(path))\n",
    "\n",
    "    # first pass over the embeddings to vocab and relevant rows\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            word, row = line.split(' ', 1)\n",
    "            if word in vocab:\n",
    "                rows.append(line)\n",
    "\n",
    "    # optionally save relevant rows to cache file.\n",
    "    if cache and not os.path.exists(cache_path):\n",
    "        with open(cache_path, 'w') as f:\n",
    "            for line in rows:\n",
    "                f.write(line)\n",
    "            print(\"Cached embeddings to {}\".format(cache_path))\n",
    "\n",
    "    # create embeddings matrix\n",
    "    embeddings = np.zeros((len(vocab), 300), dtype=np.float32)\n",
    "    for line in rows:\n",
    "        word = line.split(' ', 1)[0]\n",
    "        embeddings[vocab[word]] = list(map(float, line.strip().split(' ')[1:]))\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "embeddings = load_embeddings('glove.42B.300d.txt', vocab, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class  CNNClassifier(nn.Module):\n",
    "        \n",
    "    def __init__(self, vocab_size, embedding_dim, output_size, kernel_dim=100, kernel_sizes=(3, 4, 5), dropout=0.5):\n",
    "        super(CNNClassifier,self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, kernel_dim, (K, embedding_dim)) for K in kernel_sizes])\n",
    "\n",
    "        # kernal_size = (K,D) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * kernel_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def init_weights(self, pretrained_word_vectors, is_static=False):\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(pretrained_word_vectors).float())\n",
    "        if is_static:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, inputs, is_training=False):\n",
    "        inputs = self.embedding(inputs).unsqueeze(1) # (B,1,T,D)\n",
    "        inputs = [F.relu(conv(inputs)).squeeze(3) for conv in self.convs] #[(N,Co,W), ...]*len(Ks)\n",
    "        inputs = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in inputs] #[(N,Co), ...]*len(Ks)\n",
    "\n",
    "        concated = torch.cat(inputs, 1)\n",
    "\n",
    "        if is_training:\n",
    "            concated = self.dropout(concated) # (N,len(Ks)*Co)\n",
    "        out = self.fc(concated) \n",
    "        return F.log_softmax(out,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data(data):\n",
    "    # pad data\n",
    "    maxlen = max(max(map(len, data)), 5)\n",
    "    data = [ex + [0] * (maxlen-len(ex)) for ex in data]\n",
    "\n",
    "    # wrap in tensor\n",
    "    return torch.LongTensor(data)\n",
    "\n",
    "\n",
    "def prepare_labels(labels):\n",
    "    try:\n",
    "        return torch.LongTensor(labels)\n",
    "    except:\n",
    "        return labels\n",
    "\n",
    "\n",
    "def batch_iterator(dataset, batch_size, epoch=1):\n",
    "    dataset_size = len(dataset)\n",
    "    order = None\n",
    "    nbatches = dataset_size // batch_size\n",
    "    print(\"number of batches:\", nbatches)\n",
    "    curr_epoch=0\n",
    "\n",
    "    def init_order():\n",
    "        return random.sample(range(dataset_size), dataset_size)\n",
    "\n",
    "    def get_batch(start, end):\n",
    "        batch = [dataset[ii] for ii in order[start:end]]\n",
    "        data = prepare_data([ex['tokens'] for ex in batch])\n",
    "        labels = prepare_labels([ex['label'] for ex in batch])\n",
    "        example_ids = [1 for ex in batch]\n",
    "        return data, labels, example_ids\n",
    "\n",
    "    while True:\n",
    "        order = init_order()\n",
    "        curr_epoch += 1\n",
    "\n",
    "        for i in range(nbatches):\n",
    "            start = i*batch_size\n",
    "            end = (i+1)*batch_size\n",
    "            yield get_batch(start, end)\n",
    "\n",
    "        if nbatches*batch_size < dataset_size:\n",
    "            yield get_batch(nbatches*batch_size, dataset_size)\n",
    "\n",
    "        if curr_epoch >= epoch:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_validation(model, dataset):\n",
    "    err = 0\n",
    "    count = 0\n",
    "    for data, labels, _ in batch_iterator(dataset, 64, epoch=1):\n",
    "        if use_gpu:\n",
    "            data,labels = data.cuda(),labels.cuda()\n",
    "            \n",
    "        outp = model(Variable(data))\n",
    "        loss = nn.NLLLoss()(outp, Variable(labels))\n",
    "        acc = (outp.data.max(1)[1] == labels).sum() / data.shape[0]\n",
    "        err += (1-acc) * data.shape[0]\n",
    "        count += data.shape[0]\n",
    "    err = err / count\n",
    "    print('Ev-Err={}'.format(err))\n",
    "    return err\n",
    "\n",
    "def checkpoint_model(step, val_err, model, opt, save_path):\n",
    "    save_dict = dict(\n",
    "        step=step,\n",
    "        val_err=val_err,\n",
    "        model_state_dict=model.state_dict(),\n",
    "        opt_state_dict=opt.state_dict())\n",
    "    torch.save(save_dict, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches: 12332\n",
      "Step=0 Tr-Loss=0.7198463082313538 Tr-Acc=0.46875\n",
      "number of batches: 15\n",
      "Ev-Err=0.348\n",
      "Checkpointing model step=0 best_val_err=0.348.\n",
      "Step=100 Tr-Loss=0.49533411860466003 Tr-Acc=0.8125\n",
      "Step=200 Tr-Loss=0.5217064619064331 Tr-Acc=0.75\n",
      "Step=300 Tr-Loss=0.24462227523326874 Tr-Acc=0.9375\n",
      "Step=400 Tr-Loss=0.38554006814956665 Tr-Acc=0.859375\n",
      "Step=500 Tr-Loss=0.33596205711364746 Tr-Acc=0.859375\n",
      "Step=600 Tr-Loss=0.28589946031570435 Tr-Acc=0.90625\n",
      "Step=700 Tr-Loss=0.47102034091949463 Tr-Acc=0.8125\n",
      "Step=800 Tr-Loss=0.37264174222946167 Tr-Acc=0.828125\n",
      "Step=900 Tr-Loss=0.2897534668445587 Tr-Acc=0.859375\n",
      "Step=1000 Tr-Loss=0.3996545672416687 Tr-Acc=0.828125\n",
      "number of batches: 15\n",
      "Ev-Err=0.137\n",
      "Checkpointing model step=1000 best_val_err=0.137.\n",
      "Step=1100 Tr-Loss=0.2985065281391144 Tr-Acc=0.875\n",
      "Step=1200 Tr-Loss=0.3230428993701935 Tr-Acc=0.8125\n",
      "Step=1300 Tr-Loss=0.2329605668783188 Tr-Acc=0.921875\n",
      "Step=1400 Tr-Loss=0.3697943389415741 Tr-Acc=0.8125\n",
      "Step=1500 Tr-Loss=0.45884546637535095 Tr-Acc=0.796875\n",
      "Step=1600 Tr-Loss=0.21258817613124847 Tr-Acc=0.90625\n",
      "Step=1700 Tr-Loss=0.43111172318458557 Tr-Acc=0.84375\n",
      "Step=1800 Tr-Loss=0.17018432915210724 Tr-Acc=0.9375\n",
      "Step=1900 Tr-Loss=0.21199525892734528 Tr-Acc=0.90625\n",
      "Step=2000 Tr-Loss=0.2994368076324463 Tr-Acc=0.8125\n",
      "number of batches: 15\n",
      "Ev-Err=0.124\n",
      "Checkpointing model step=2000 best_val_err=0.124.\n",
      "Step=2100 Tr-Loss=0.36895880103111267 Tr-Acc=0.859375\n",
      "Step=2200 Tr-Loss=0.28060853481292725 Tr-Acc=0.875\n",
      "Step=2300 Tr-Loss=0.3014492690563202 Tr-Acc=0.828125\n",
      "Step=2400 Tr-Loss=0.2095608115196228 Tr-Acc=0.90625\n",
      "Step=2500 Tr-Loss=0.3201609253883362 Tr-Acc=0.875\n",
      "Step=2600 Tr-Loss=0.24056454002857208 Tr-Acc=0.90625\n",
      "Step=2700 Tr-Loss=0.26935875415802 Tr-Acc=0.875\n",
      "Step=2800 Tr-Loss=0.3386959731578827 Tr-Acc=0.859375\n",
      "Step=2900 Tr-Loss=0.1517547369003296 Tr-Acc=0.9375\n",
      "Step=3000 Tr-Loss=0.349763959646225 Tr-Acc=0.859375\n",
      "number of batches: 15\n",
      "Ev-Err=0.118\n",
      "Checkpointing model step=3000 best_val_err=0.118.\n",
      "Step=3100 Tr-Loss=0.20959866046905518 Tr-Acc=0.9375\n",
      "Step=3200 Tr-Loss=0.2077757567167282 Tr-Acc=0.9375\n",
      "Step=3300 Tr-Loss=0.2644175887107849 Tr-Acc=0.875\n",
      "Step=3400 Tr-Loss=0.33064424991607666 Tr-Acc=0.8125\n",
      "Step=3500 Tr-Loss=0.36237832903862 Tr-Acc=0.828125\n",
      "Step=3600 Tr-Loss=0.17089222371578217 Tr-Acc=0.953125\n",
      "Step=3700 Tr-Loss=0.30774712562561035 Tr-Acc=0.875\n",
      "Step=3800 Tr-Loss=0.24666576087474823 Tr-Acc=0.921875\n",
      "Step=3900 Tr-Loss=0.2638185918331146 Tr-Acc=0.859375\n",
      "Step=4000 Tr-Loss=0.29120969772338867 Tr-Acc=0.875\n",
      "number of batches: 15\n",
      "Ev-Err=0.105\n",
      "Checkpointing model step=4000 best_val_err=0.105.\n",
      "Step=4100 Tr-Loss=0.399980753660202 Tr-Acc=0.875\n",
      "Step=4200 Tr-Loss=0.29500994086265564 Tr-Acc=0.875\n",
      "Step=4300 Tr-Loss=0.302580863237381 Tr-Acc=0.859375\n",
      "Step=4400 Tr-Loss=0.16166310012340546 Tr-Acc=0.9375\n",
      "Step=4500 Tr-Loss=0.19922557473182678 Tr-Acc=0.953125\n",
      "Step=4600 Tr-Loss=0.3207643926143646 Tr-Acc=0.859375\n",
      "Step=4700 Tr-Loss=0.1977158933877945 Tr-Acc=0.9375\n",
      "Step=4800 Tr-Loss=0.3026503920555115 Tr-Acc=0.875\n",
      "Step=4900 Tr-Loss=0.09392277896404266 Tr-Acc=0.96875\n",
      "Step=5000 Tr-Loss=0.2215518355369568 Tr-Acc=0.890625\n",
      "number of batches: 15\n",
      "Ev-Err=0.104\n",
      "Checkpointing model step=5000 best_val_err=0.104.\n",
      "Step=5100 Tr-Loss=0.27321454882621765 Tr-Acc=0.890625\n",
      "Step=5200 Tr-Loss=0.35059642791748047 Tr-Acc=0.890625\n",
      "Step=5300 Tr-Loss=0.3406285047531128 Tr-Acc=0.921875\n",
      "Step=5400 Tr-Loss=0.2604221999645233 Tr-Acc=0.90625\n",
      "Step=5500 Tr-Loss=0.2547701895236969 Tr-Acc=0.859375\n",
      "Step=5600 Tr-Loss=0.36003875732421875 Tr-Acc=0.875\n",
      "Step=5700 Tr-Loss=0.24980445206165314 Tr-Acc=0.890625\n",
      "Step=5800 Tr-Loss=0.31517651677131653 Tr-Acc=0.875\n",
      "Step=5900 Tr-Loss=0.2523806393146515 Tr-Acc=0.875\n",
      "Step=6000 Tr-Loss=0.24640488624572754 Tr-Acc=0.90625\n",
      "number of batches: 15\n",
      "Ev-Err=0.109\n",
      "Step=6100 Tr-Loss=0.254189133644104 Tr-Acc=0.921875\n",
      "Step=6200 Tr-Loss=0.29666629433631897 Tr-Acc=0.875\n",
      "Step=6300 Tr-Loss=0.3255962133407593 Tr-Acc=0.84375\n",
      "Step=6400 Tr-Loss=0.4510546028614044 Tr-Acc=0.875\n",
      "Step=6500 Tr-Loss=0.32035231590270996 Tr-Acc=0.859375\n",
      "Step=6600 Tr-Loss=0.2996288239955902 Tr-Acc=0.921875\n",
      "Step=6700 Tr-Loss=0.2953401803970337 Tr-Acc=0.890625\n",
      "Step=6800 Tr-Loss=0.2786725163459778 Tr-Acc=0.859375\n",
      "Step=6900 Tr-Loss=0.22346852719783783 Tr-Acc=0.9375\n",
      "Step=7000 Tr-Loss=0.4663625955581665 Tr-Acc=0.796875\n",
      "number of batches: 15\n",
      "Ev-Err=0.112\n",
      "Step=7100 Tr-Loss=0.2517329752445221 Tr-Acc=0.859375\n",
      "Step=7200 Tr-Loss=0.24855677783489227 Tr-Acc=0.890625\n",
      "Step=7300 Tr-Loss=0.30111196637153625 Tr-Acc=0.90625\n",
      "Step=7400 Tr-Loss=0.2776135206222534 Tr-Acc=0.859375\n",
      "Step=7500 Tr-Loss=0.235691636800766 Tr-Acc=0.90625\n",
      "Step=7600 Tr-Loss=0.1857280731201172 Tr-Acc=0.921875\n",
      "Step=7700 Tr-Loss=0.28980568051338196 Tr-Acc=0.84375\n",
      "Step=7800 Tr-Loss=0.19761356711387634 Tr-Acc=0.9375\n",
      "Step=7900 Tr-Loss=0.17321135103702545 Tr-Acc=0.953125\n",
      "Step=8000 Tr-Loss=0.3518165647983551 Tr-Acc=0.859375\n",
      "number of batches: 15\n",
      "Ev-Err=0.105\n",
      "Step=8100 Tr-Loss=0.23764954507350922 Tr-Acc=0.875\n",
      "Step=8200 Tr-Loss=0.23827774822711945 Tr-Acc=0.890625\n",
      "Step=8300 Tr-Loss=0.20042915642261505 Tr-Acc=0.890625\n",
      "Step=8400 Tr-Loss=0.22725556790828705 Tr-Acc=0.890625\n",
      "Step=8500 Tr-Loss=0.20585349202156067 Tr-Acc=0.90625\n",
      "Step=8600 Tr-Loss=0.3967320919036865 Tr-Acc=0.84375\n",
      "Step=8700 Tr-Loss=0.19015240669250488 Tr-Acc=0.9375\n",
      "Step=8800 Tr-Loss=0.22765402495861053 Tr-Acc=0.90625\n",
      "Step=8900 Tr-Loss=0.2706179916858673 Tr-Acc=0.890625\n",
      "Step=9000 Tr-Loss=0.08520827442407608 Tr-Acc=0.984375\n",
      "number of batches: 15\n",
      "Ev-Err=0.11\n",
      "Step=9100 Tr-Loss=0.16910284757614136 Tr-Acc=0.921875\n",
      "Step=9200 Tr-Loss=0.31297916173934937 Tr-Acc=0.859375\n",
      "Step=9300 Tr-Loss=0.11352522671222687 Tr-Acc=0.96875\n",
      "Step=9400 Tr-Loss=0.255662739276886 Tr-Acc=0.875\n",
      "Step=9500 Tr-Loss=0.206009179353714 Tr-Acc=0.9375\n",
      "Step=9600 Tr-Loss=0.21641047298908234 Tr-Acc=0.90625\n",
      "Step=9700 Tr-Loss=0.298533171415329 Tr-Acc=0.875\n",
      "Step=9800 Tr-Loss=0.2851620018482208 Tr-Acc=0.921875\n",
      "Step=9900 Tr-Loss=0.38696756958961487 Tr-Acc=0.859375\n",
      "Step=10000 Tr-Loss=0.25925666093826294 Tr-Acc=0.890625\n",
      "number of batches: 15\n",
      "Ev-Err=0.108\n",
      "Step=10100 Tr-Loss=0.25684744119644165 Tr-Acc=0.90625\n",
      "Step=10200 Tr-Loss=0.46166354417800903 Tr-Acc=0.859375\n",
      "Step=10300 Tr-Loss=0.21437205374240875 Tr-Acc=0.921875\n",
      "Step=10400 Tr-Loss=0.33866411447525024 Tr-Acc=0.796875\n",
      "Step=10500 Tr-Loss=0.33015313744544983 Tr-Acc=0.921875\n",
      "Step=10600 Tr-Loss=0.23576056957244873 Tr-Acc=0.921875\n",
      "Step=10700 Tr-Loss=0.16349215805530548 Tr-Acc=0.90625\n",
      "Step=10800 Tr-Loss=0.24191130697727203 Tr-Acc=0.921875\n",
      "Step=10900 Tr-Loss=0.25550377368927 Tr-Acc=0.875\n",
      "Step=11000 Tr-Loss=0.15946733951568604 Tr-Acc=0.9375\n",
      "number of batches: 15\n",
      "Ev-Err=0.107\n",
      "Step=11100 Tr-Loss=0.24517256021499634 Tr-Acc=0.890625\n",
      "Step=11200 Tr-Loss=0.20809626579284668 Tr-Acc=0.9375\n",
      "Step=11300 Tr-Loss=0.20561324059963226 Tr-Acc=0.921875\n",
      "Step=11400 Tr-Loss=0.2874268591403961 Tr-Acc=0.859375\n",
      "Step=11500 Tr-Loss=0.28994080424308777 Tr-Acc=0.84375\n",
      "Step=11600 Tr-Loss=0.2472507357597351 Tr-Acc=0.890625\n",
      "Step=11700 Tr-Loss=0.27177006006240845 Tr-Acc=0.859375\n",
      "Step=11800 Tr-Loss=0.2771230638027191 Tr-Acc=0.84375\n",
      "Step=11900 Tr-Loss=0.2897949516773224 Tr-Acc=0.890625\n",
      "Step=12000 Tr-Loss=0.1292111724615097 Tr-Acc=0.96875\n",
      "number of batches: 15\n",
      "Ev-Err=0.108\n",
      "Step=12100 Tr-Loss=0.26179039478302 Tr-Acc=0.890625\n",
      "Step=12200 Tr-Loss=0.3468497693538666 Tr-Acc=0.84375\n",
      "Step=12300 Tr-Loss=0.22805076837539673 Tr-Acc=0.875\n",
      "Step=12400 Tr-Loss=0.2797964811325073 Tr-Acc=0.875\n",
      "Step=12500 Tr-Loss=0.276746928691864 Tr-Acc=0.890625\n",
      "Step=12600 Tr-Loss=0.25250229239463806 Tr-Acc=0.890625\n",
      "Step=12700 Tr-Loss=0.3020048141479492 Tr-Acc=0.796875\n",
      "Step=12800 Tr-Loss=0.19428607821464539 Tr-Acc=0.9375\n",
      "Step=12900 Tr-Loss=0.3249604403972626 Tr-Acc=0.84375\n",
      "Step=13000 Tr-Loss=0.2871250510215759 Tr-Acc=0.828125\n",
      "number of batches: 15\n",
      "Ev-Err=0.104\n",
      "Step=13100 Tr-Loss=0.23754405975341797 Tr-Acc=0.890625\n",
      "Step=13200 Tr-Loss=0.25523439049720764 Tr-Acc=0.921875\n",
      "Step=13300 Tr-Loss=0.2651917040348053 Tr-Acc=0.890625\n",
      "Step=13400 Tr-Loss=0.2585740089416504 Tr-Acc=0.890625\n",
      "Step=13500 Tr-Loss=0.20346780121326447 Tr-Acc=0.921875\n",
      "Step=13600 Tr-Loss=0.17013023793697357 Tr-Acc=0.9375\n",
      "Step=13700 Tr-Loss=0.2355647087097168 Tr-Acc=0.890625\n",
      "Step=13800 Tr-Loss=0.11509628593921661 Tr-Acc=0.953125\n",
      "Step=13900 Tr-Loss=0.1511230170726776 Tr-Acc=0.953125\n",
      "Step=14000 Tr-Loss=0.26356831192970276 Tr-Acc=0.890625\n",
      "number of batches: 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ev-Err=0.101\n",
      "Checkpointing model step=14000 best_val_err=0.101.\n",
      "Step=14100 Tr-Loss=0.19340704381465912 Tr-Acc=0.921875\n",
      "Step=14200 Tr-Loss=0.17206670343875885 Tr-Acc=0.9375\n",
      "Step=14300 Tr-Loss=0.15311965346336365 Tr-Acc=0.953125\n",
      "Step=14400 Tr-Loss=0.19227932393550873 Tr-Acc=0.921875\n",
      "Step=14500 Tr-Loss=0.16396480798721313 Tr-Acc=0.9375\n",
      "Step=14600 Tr-Loss=0.2885359227657318 Tr-Acc=0.875\n",
      "Step=14700 Tr-Loss=0.2151484191417694 Tr-Acc=0.890625\n",
      "Step=14800 Tr-Loss=0.21198344230651855 Tr-Acc=0.9375\n",
      "Step=14900 Tr-Loss=0.2969956696033478 Tr-Acc=0.890625\n",
      "Step=15000 Tr-Loss=0.25774499773979187 Tr-Acc=0.890625\n",
      "number of batches: 15\n",
      "Ev-Err=0.105\n",
      "Step=15100 Tr-Loss=0.20557576417922974 Tr-Acc=0.90625\n",
      "Step=15200 Tr-Loss=0.42879781126976013 Tr-Acc=0.859375\n",
      "Step=15300 Tr-Loss=0.21409951150417328 Tr-Acc=0.90625\n",
      "Step=15400 Tr-Loss=0.13900305330753326 Tr-Acc=0.96875\n",
      "Step=15500 Tr-Loss=0.2906314730644226 Tr-Acc=0.90625\n",
      "Step=15600 Tr-Loss=0.37191593647003174 Tr-Acc=0.90625\n",
      "Step=15700 Tr-Loss=0.1431935429573059 Tr-Acc=0.953125\n",
      "Step=15800 Tr-Loss=0.30761411786079407 Tr-Acc=0.84375\n",
      "Step=15900 Tr-Loss=0.1821283996105194 Tr-Acc=0.921875\n",
      "Step=16000 Tr-Loss=0.14317600429058075 Tr-Acc=0.9375\n",
      "number of batches: 15\n",
      "Ev-Err=0.098\n",
      "Checkpointing model step=16000 best_val_err=0.098.\n",
      "Step=16100 Tr-Loss=0.17604303359985352 Tr-Acc=0.921875\n",
      "Step=16200 Tr-Loss=0.3621211349964142 Tr-Acc=0.859375\n",
      "Step=16300 Tr-Loss=0.27131322026252747 Tr-Acc=0.90625\n",
      "Step=16400 Tr-Loss=0.1653427928686142 Tr-Acc=0.9375\n",
      "Step=16500 Tr-Loss=0.31304270029067993 Tr-Acc=0.859375\n",
      "Step=16600 Tr-Loss=0.34373739361763 Tr-Acc=0.875\n",
      "Step=16700 Tr-Loss=0.20514148473739624 Tr-Acc=0.90625\n",
      "Step=16800 Tr-Loss=0.2005710005760193 Tr-Acc=0.875\n",
      "Step=16900 Tr-Loss=0.4363533556461334 Tr-Acc=0.75\n",
      "Step=17000 Tr-Loss=0.27736347913742065 Tr-Acc=0.921875\n",
      "number of batches: 15\n",
      "Ev-Err=0.095\n",
      "Checkpointing model step=17000 best_val_err=0.095.\n",
      "Step=17100 Tr-Loss=0.3256760537624359 Tr-Acc=0.90625\n",
      "Step=17200 Tr-Loss=0.3303053677082062 Tr-Acc=0.890625\n",
      "Step=17300 Tr-Loss=0.18664397299289703 Tr-Acc=0.921875\n",
      "Step=17400 Tr-Loss=0.24130456149578094 Tr-Acc=0.875\n",
      "Step=17500 Tr-Loss=0.3787817656993866 Tr-Acc=0.84375\n",
      "Step=17600 Tr-Loss=0.24421605467796326 Tr-Acc=0.890625\n",
      "Step=17700 Tr-Loss=0.13746142387390137 Tr-Acc=0.96875\n",
      "Step=17800 Tr-Loss=0.31959283351898193 Tr-Acc=0.875\n",
      "Step=17900 Tr-Loss=0.2532043159008026 Tr-Acc=0.890625\n",
      "Step=18000 Tr-Loss=0.22801460325717926 Tr-Acc=0.90625\n",
      "number of batches: 15\n",
      "Ev-Err=0.102\n",
      "Step=18100 Tr-Loss=0.2179630696773529 Tr-Acc=0.921875\n",
      "Step=18200 Tr-Loss=0.18273186683654785 Tr-Acc=0.953125\n",
      "Step=18300 Tr-Loss=0.1571509838104248 Tr-Acc=0.953125\n",
      "Step=18400 Tr-Loss=0.26699188351631165 Tr-Acc=0.875\n",
      "Step=18500 Tr-Loss=0.25632795691490173 Tr-Acc=0.890625\n",
      "Step=18600 Tr-Loss=0.12371182441711426 Tr-Acc=0.9375\n",
      "Step=18700 Tr-Loss=0.24794012308120728 Tr-Acc=0.921875\n",
      "Step=18800 Tr-Loss=0.16058064997196198 Tr-Acc=0.9375\n",
      "Step=18900 Tr-Loss=0.25723761320114136 Tr-Acc=0.890625\n",
      "Step=19000 Tr-Loss=0.2539004683494568 Tr-Acc=0.90625\n",
      "number of batches: 15\n",
      "Ev-Err=0.099\n",
      "Step=19100 Tr-Loss=0.16788877546787262 Tr-Acc=0.9375\n",
      "Step=19200 Tr-Loss=0.21616165339946747 Tr-Acc=0.90625\n",
      "Step=19300 Tr-Loss=0.16781912744045258 Tr-Acc=0.953125\n",
      "Step=19400 Tr-Loss=0.13208188116550446 Tr-Acc=0.9375\n",
      "Step=19500 Tr-Loss=0.15565413236618042 Tr-Acc=0.9375\n",
      "Step=19600 Tr-Loss=0.10955968499183655 Tr-Acc=0.984375\n",
      "Step=19700 Tr-Loss=0.37384843826293945 Tr-Acc=0.875\n",
      "Step=19800 Tr-Loss=0.20679135620594025 Tr-Acc=0.953125\n",
      "Step=19900 Tr-Loss=0.21678955852985382 Tr-Acc=0.9375\n",
      "Step=20000 Tr-Loss=0.18540892004966736 Tr-Acc=0.9375\n",
      "number of batches: 15\n",
      "Ev-Err=0.109\n",
      "Step=20100 Tr-Loss=0.2698696553707123 Tr-Acc=0.8125\n",
      "Step=20200 Tr-Loss=0.27981725335121155 Tr-Acc=0.90625\n",
      "Step=20300 Tr-Loss=0.3318076431751251 Tr-Acc=0.84375\n",
      "Step=20400 Tr-Loss=0.13541574776172638 Tr-Acc=0.953125\n",
      "Step=20500 Tr-Loss=0.2786844074726105 Tr-Acc=0.859375\n",
      "Step=20600 Tr-Loss=0.3977023661136627 Tr-Acc=0.828125\n",
      "Step=20700 Tr-Loss=0.27434179186820984 Tr-Acc=0.859375\n",
      "Step=20800 Tr-Loss=0.2637888193130493 Tr-Acc=0.890625\n",
      "Step=20900 Tr-Loss=0.17973187565803528 Tr-Acc=0.921875\n",
      "Step=21000 Tr-Loss=0.1730489581823349 Tr-Acc=0.953125\n",
      "number of batches: 15\n",
      "Ev-Err=0.101\n",
      "Step=21100 Tr-Loss=0.16713401675224304 Tr-Acc=0.921875\n",
      "Step=21200 Tr-Loss=0.36717984080314636 Tr-Acc=0.828125\n",
      "Step=21300 Tr-Loss=0.31146860122680664 Tr-Acc=0.890625\n",
      "Step=21400 Tr-Loss=0.36304178833961487 Tr-Acc=0.859375\n",
      "Step=21500 Tr-Loss=0.1981649100780487 Tr-Acc=0.890625\n",
      "Step=21600 Tr-Loss=0.1632259339094162 Tr-Acc=0.90625\n",
      "Step=21700 Tr-Loss=0.13745087385177612 Tr-Acc=0.953125\n",
      "Step=21800 Tr-Loss=0.21583732962608337 Tr-Acc=0.921875\n",
      "Step=21900 Tr-Loss=0.33508461713790894 Tr-Acc=0.84375\n",
      "Step=22000 Tr-Loss=0.212275892496109 Tr-Acc=0.921875\n",
      "number of batches: 15\n",
      "Ev-Err=0.1\n",
      "Step=22100 Tr-Loss=0.25076472759246826 Tr-Acc=0.90625\n",
      "Step=22200 Tr-Loss=0.17729170620441437 Tr-Acc=0.921875\n",
      "Step=22300 Tr-Loss=0.3525152802467346 Tr-Acc=0.828125\n",
      "Step=22400 Tr-Loss=0.228201761841774 Tr-Acc=0.953125\n",
      "Step=22500 Tr-Loss=0.28384801745414734 Tr-Acc=0.875\n",
      "Step=22600 Tr-Loss=0.34519603848457336 Tr-Acc=0.875\n",
      "Step=22700 Tr-Loss=0.29746121168136597 Tr-Acc=0.890625\n",
      "Step=22800 Tr-Loss=0.3702526092529297 Tr-Acc=0.875\n",
      "Step=22900 Tr-Loss=0.17842622101306915 Tr-Acc=0.921875\n",
      "Step=23000 Tr-Loss=0.17754973471164703 Tr-Acc=0.953125\n",
      "number of batches: 15\n",
      "Ev-Err=0.097\n",
      "Step=23100 Tr-Loss=0.2611505687236786 Tr-Acc=0.875\n",
      "Step=23200 Tr-Loss=0.20530155301094055 Tr-Acc=0.921875\n",
      "Step=23300 Tr-Loss=0.24143075942993164 Tr-Acc=0.890625\n",
      "Step=23400 Tr-Loss=0.3712628185749054 Tr-Acc=0.828125\n",
      "Step=23500 Tr-Loss=0.2612476050853729 Tr-Acc=0.875\n",
      "Step=23600 Tr-Loss=0.20493608713150024 Tr-Acc=0.953125\n",
      "Step=23700 Tr-Loss=0.2521297335624695 Tr-Acc=0.875\n",
      "Step=23800 Tr-Loss=0.36208420991897583 Tr-Acc=0.828125\n",
      "Step=23900 Tr-Loss=0.12323186546564102 Tr-Acc=0.953125\n",
      "Step=24000 Tr-Loss=0.1957182139158249 Tr-Acc=0.921875\n",
      "number of batches: 15\n",
      "Ev-Err=0.1\n",
      "Step=24100 Tr-Loss=0.28163278102874756 Tr-Acc=0.921875\n",
      "Step=24200 Tr-Loss=0.22396793961524963 Tr-Acc=0.90625\n",
      "Step=24300 Tr-Loss=0.28114303946495056 Tr-Acc=0.859375\n",
      "Step=24400 Tr-Loss=0.2404048591852188 Tr-Acc=0.921875\n",
      "Step=24500 Tr-Loss=0.21997414529323578 Tr-Acc=0.921875\n",
      "Step=24600 Tr-Loss=0.17955757677555084 Tr-Acc=0.90625\n",
      "Step=24700 Tr-Loss=0.18261116743087769 Tr-Acc=0.953125\n",
      "Step=24800 Tr-Loss=0.2753821313381195 Tr-Acc=0.90625\n",
      "Step=24900 Tr-Loss=0.3558710515499115 Tr-Acc=0.828125\n",
      "Step=25000 Tr-Loss=0.20449918508529663 Tr-Acc=0.875\n",
      "number of batches: 15\n",
      "Ev-Err=0.095\n",
      "Step=25100 Tr-Loss=0.25893843173980713 Tr-Acc=0.890625\n",
      "Step=25200 Tr-Loss=0.15540647506713867 Tr-Acc=0.921875\n",
      "Step=25300 Tr-Loss=0.3101595938205719 Tr-Acc=0.859375\n",
      "Step=25400 Tr-Loss=0.24427530169487 Tr-Acc=0.90625\n",
      "Step=25500 Tr-Loss=0.13808505237102509 Tr-Acc=0.9375\n",
      "Step=25600 Tr-Loss=0.20418860018253326 Tr-Acc=0.9375\n",
      "Step=25700 Tr-Loss=0.276699423789978 Tr-Acc=0.921875\n",
      "Step=25800 Tr-Loss=0.08491280674934387 Tr-Acc=0.96875\n",
      "Step=25900 Tr-Loss=0.11568762362003326 Tr-Acc=0.953125\n",
      "Step=26000 Tr-Loss=0.3484560251235962 Tr-Acc=0.875\n",
      "number of batches: 15\n",
      "Ev-Err=0.093\n",
      "Checkpointing model step=26000 best_val_err=0.093.\n",
      "Step=26100 Tr-Loss=0.19675643742084503 Tr-Acc=0.9375\n",
      "Step=26200 Tr-Loss=0.24452465772628784 Tr-Acc=0.921875\n",
      "Step=26300 Tr-Loss=0.2206408828496933 Tr-Acc=0.90625\n",
      "Step=26400 Tr-Loss=0.2963429093360901 Tr-Acc=0.84375\n",
      "Step=26500 Tr-Loss=0.3201506435871124 Tr-Acc=0.890625\n",
      "Step=26600 Tr-Loss=0.29977643489837646 Tr-Acc=0.84375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-52cc3e95f1c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "train_data, validation_data, test_data = train, validate, test\n",
    "\n",
    "model = CNNClassifier(*embeddings.shape, 2)\n",
    "model.init_weights(embeddings, True)\n",
    "\n",
    "if use_gpu: model = model.cuda()\n",
    "\n",
    "opt = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-4)\n",
    "step = 0\n",
    "best_val_err = 1\n",
    "\n",
    "for data, labels, example_ids in batch_iterator(train_data, 64, epoch=10):\n",
    "    if use_gpu:\n",
    "        data, labels = data.cuda(),labels.cuda()\n",
    "\n",
    "    outp = model(Variable(data),True)\n",
    "    loss = nn.NLLLoss()(outp,Variable(labels))\n",
    "    acc = (outp.data.max(1)[1]== labels).sum()/data.shape[0]\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print('Step={} Tr-Loss={} Tr-Acc={}'.format(step, loss.data[0], acc))\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        val_err = run_validation(model, validation_data[:1000])\n",
    "#         save_path(step, val_err)\n",
    "\n",
    "        # early stopping\n",
    "        if val_err < best_val_err:\n",
    "            best_val_err = val_err\n",
    "            print('Checkpointing model step={} best_val_err={}.'.format(step, best_val_err))\n",
    "            checkpoint_model(step, val_err, model, opt, 'model.ckpt')\n",
    "#             save_validation(model, validation_data, options)\n",
    "\n",
    "    step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model.ckpt\n",
      "step=26000 best_val_err=0.093\n",
      "number of batches: 2055\n",
      "Ev-Err=0.09007009168174424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09007009168174424"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_model(model, opt, load_path):\n",
    "    load_dict = torch.load(load_path)\n",
    "    step = load_dict['step']\n",
    "    val_err = load_dict['val_err']\n",
    "    model.load_state_dict(load_dict['model_state_dict'])\n",
    "    opt.load_state_dict(load_dict['opt_state_dict'])\n",
    "    return step, val_err\n",
    "\n",
    "step, best_val_err = load_model(model, opt, 'model.ckpt')\n",
    "print('Model loaded from {}\\nstep={} best_val_err={}'.format('model.ckpt', step, best_val_err))\n",
    "val_err = run_validation(model, validation_data)\n",
    "val_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.append(np.array([1]),np.array([1,2]))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8713338705508507\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = train['text'],train['positive']\n",
    "test_data, test_labels = validate['text'],validate['positive']\n",
    "\n",
    "# LogisticRegression, MultinomialNB, MLPClassifier\n",
    "# , ngram_range=(1,2), max_features=20000\n",
    "# SVC(gamma=2, C=1),\n",
    "# QuadraticDiscriminantAnalysis()\n",
    "# Perceptron(n_jobs=-1,tol=1e-4,max_iter=1000)\n",
    "text_clf = Pipeline([('vec', CountVectorizer(stop_words='english'),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('model', LogisticRegression()),\n",
    "])\n",
    "\n",
    "text_clf = text_clf.fit(train_data, train_labels)\n",
    "\n",
    "predicted = text_clf.predict(test_data)\n",
    "\n",
    "print(\"accuracy:\", np.mean(predicted == test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.828605586349\n",
      "2 0.234343247737\n",
      "3 0.3082620355\n",
      "4 0.473060202872\n",
      "5 0.875746162592\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    print(i, mean(predicted[test_labels==i]==i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
